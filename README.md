# ADS-509_Final_Project: Categorizing Journal Articles

This project is a part of the ADS-509 (Text Mining) course in the Applied Data Science Program at the University of San Diego. 

-- Project Status: [Complete]

## Installation
To use this project, first clone the repo on your device using the command below:
git init
git clone https://github.com/ADS-509-01-SP23-Applied-Text-Mining/ADS-509_Final_Project.git

### Project Intro/Objective
We will take the needed data from arXiv, tokenize, and normalize the text then develop a model to classify articles based on the tokens we discover.  

### Partner(s)/Contributor(s)  
*	Zachariah Freitas 
*	Brianne Bell 
*	Data sourced from: : https://arxiv.org/  

### Methods Used
A few examples are:
*	Tokenization
* Normalization
*	API	
* Pickle
*	Data Visualization
  * WordCloud
* Topic Modeling
  * NMF - Nonnegative Matrix Factorization 
  * LDA - Latent Dirichlet Allocation
  * LSA - Latent Semantic Analysis
* Classification Modeling
  * Multinomial Logistic Regression
  * Multinomial Naive Bayes
* AUC ROC Curve

### Technologies
*	Python 3
* Google Drive
  * storage of pickled file for retrieval in descriptive statistics and modeling phases

## Project Description
We are looking at Cornell Universityâ€™s arXiv (open access archive) of scholarly articles. The website has 2,200,384 scholarly articles in various fields of study, though primarily in STEM and financial fields. We will focus on titles and abstracts then collect key words from those instead of entire articles.  

## License
Data was open-source data

## Acknowledgments
We are grateful for Professor John Chandler for timely and informative feedback and assistance.
